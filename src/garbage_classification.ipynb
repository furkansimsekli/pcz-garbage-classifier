{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garbage Classification\n",
    "\n",
    "![Garbage Bins](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwebstockreview.net%2Fimages%2Fgarbage-clipart-wastebin-16.png&f=1&nofb=1)\n",
    "\n",
    "\n",
    "Garbage segregation involves separating wastes according to how it's handled or processed. It's important for recycling as some materials are recyclable and others are not.\n",
    "\n",
    "In this project I will use PyTorch for classifying garbage into various categories like metal, paper, glass, plastic, cardboard and trash. The dataset I am going to use is free to use and [available on Kaggle](https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification).\n",
    "\n",
    "While working on this project I will try different neural network architectures, different hyperparameters and different variations of dataset. However, for the sake of the completeness of the notebook, I won't be putting everything here. I will mention different results and the known reasons behind them in different places throughout the notebook.\n",
    "\n",
    "Please note that, due to magic commands and iPython's capabilities (printing out a variable by just typing the variable) this notebook is not compatible with ordinary Python. Be aware of this if you are going to export it as `.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path  = \"../resources/garbage_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset from given folder. Pytorch's [ImageFolder](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html) method makes our job really easy in this case. `dataset.classes` shows the available categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(dataset_path)\n",
    "num_classes = len(dataset.classes)\n",
    "dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations & Augmentation\n",
    "\n",
    "Increasing the number of inputs helps for our model to learn better. I won't be increasing the number of images in our filesystem, but PyTorch's transformations will help us to increase it by giving out random flipped or random rotated images from loaders.\n",
    "\n",
    "Also neural networks tend to learn better when the pixel values are not between $0-255$ but $0-1$, although this time I will normalize the pixel values by applying normalization to the images by the following formula. Mean and standard deviation values belongs to [ImageNet](https://www.image-net.org/index.php). I could have calculated from our own dataset, but this also works fine for us.\n",
    "\n",
    "$$\n",
    "    output = \\frac{input - std}{mean}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(ds, idxs):\n",
    "    \"\"\"\n",
    "    Takes dataset, and number of samples to show. Shows them on a plot with their\n",
    "    corresponding labels.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 2))\n",
    "\n",
    "    for i, idx in enumerate(idxs):\n",
    "        img, label = ds[idx]\n",
    "\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            axes[i].imshow(img.permute(1, 2, 0))\n",
    "        else:\n",
    "            axes[i].imshow(img)\n",
    "\n",
    "        axes[i].set_title(f\"Label: {dataset.classes[label]}\")\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show samples before transformations being applied to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 6\n",
    "idxs = torch.randint(len(dataset), size=(num_samples,))\n",
    "show_samples(dataset, idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the same samples after the transformations applied.\n",
    "\n",
    "Please ignore the warning message from matplotlib. These warning messages occurs because after normalization, some of the pixel values goes out the range, such as they become negative values. That's why matplotlib clips those invalid values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean = (0.485, 0.456, 0.406)\n",
    "imagenet_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "transformations = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Resize((256, 256), antialias=True),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomRotation(40),\n",
    "    v2.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(dataset_path, transform=transformations)\n",
    "show_samples(dataset, idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Splitting Data for Training, Validation and Testing\n",
    "\n",
    "I will use 85% of the total data for training, 10% for validation and 5% for testing. These numbers may vary in different applications, but the common convention is that size of the training should be much larger than the rest. Since validation will tell me where to stop before overfitting, I want it to be bigger than test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use a random seed in order to get consistent results in every ty\n",
    "# However, it's not a big deal, so you can turn it off if you want!\n",
    "random_seed = 42\n",
    "generator = torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = random_split(dataset, [0.85, 0.10, 0.05], generator=generator)\n",
    "print(f\"Train set size: {len(train_ds)}\")\n",
    "print(f\"Validation set size: {len(val_ds)}\")\n",
    "print(f\"Test set size: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_dl = DataLoader(val_ds, batch_size, num_workers=num_workers)\n",
    "test_dl = DataLoader(test_ds, batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    \"\"\"\n",
    "    Shows the first batch from a given DataLoader.\n",
    "    \"\"\"\n",
    "    images, labels = next(iter(dl))\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(make_grid(images, nrow = 16).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Model base will help us to train and validate our model. Thanks to this way, we won't need to write everything inside of the training loop. Later on, we will create our architecture while inheriting this base class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy by comparing the outputs with the true labels.\n",
    "    \"\"\"\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        \"\"\"\n",
    "        Takes the training batch, calculates loss and accuracy.\n",
    "        \"\"\"\n",
    "        images, labels = batch \n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = calculate_accuracy(out, labels)\n",
    "        return loss, acc\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        \"\"\"\n",
    "        Takes the validation batch, calculates loss and accuracy. Returns a dictionary of loss and accuracy.\n",
    "        \"\"\"\n",
    "        images, labels = batch \n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = calculate_accuracy(out, labels)\n",
    "        return {\"val_loss\": loss.detach(), \"val_acc\": acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Takes the outputs from all of the batches, and calculates average loss and accuracy.\n",
    "        Returns a dictionary of average loss and accuracy.\n",
    "        \"\"\"\n",
    "        batch_losses = [x[\"val_loss\"] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x[\"val_acc\"] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {\"val_loss\": epoch_loss.item(), \"val_acc\": epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        \"\"\"\n",
    "        Takes epoch number, and results after both training and validation. Prints out the values.\n",
    "        \"\"\"\n",
    "        print(\"Epoch {}: train_acc: {:.4f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch + 1, result[\"train_acc\"], result[\"train_loss\"], result[\"val_loss\"], result[\"val_acc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyNet\n",
    "\n",
    "The network so called `MyNet` below is made up from 5 convolutional layers for feature extraction, and for the classification usual linear layers have been used. The whole architecture looks like [AlexNet](https://en.wikipedia.org/wiki/AlexNet) who won several contests like the first ImageNet contest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(ImageClassificationBase):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "summary(MyNet(len(dataset.classes)), (3, 256, 256));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### StarkNet\n",
    "\n",
    "The architecture below utilizes mostly [Batch Normalization](https://arxiv.org/abs/1502.03167) instead of Max Pooling. It's also one of the networks I have used for this dataset. It gives slightly better results compared to `MyNet`. Batch Normalization's intention is actually not give better results but [improve the speed of the training with fewer steps](https://arxiv.org/abs/1502.03167). I'm thinking for this dataset, I'm a bit lucky for getting a better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StarkNet(ImageClassificationBase):\n",
    "    def __init__(self, num_classes):\n",
    "        super(StarkNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(128),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "summary(StarkNet(len(dataset.classes)), (3, 256, 256));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Resnet\n",
    "\n",
    "In order to see what we can achieve with a near perfect model, I will also finetune Resnet50 and use it. It's not unlikely to see **95%** accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class ResNet(ImageClassificationBase):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.network = models.resnet50(weights=True)\n",
    "        num_ftrs = self.network.fc.in_features\n",
    "        self.network.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return torch.sigmoid(self.network(xb))\n",
    "\n",
    "summary(ResNet(len(dataset.classes)), (3, 256, 256));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porting to GPU\n",
    "\n",
    "GPUs tend to perform faster calculations than CPU. Although my machine doesn't have a GPU, I will still write the code so that we can utilize GPU if we have in environments like Google Colab. Using a custom Data Loader will help us when we need to move the images to device memory. This is again in order to reduce the complexity in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"\n",
    "    Returns the utilized device in the host machine. If you are using CUDA and but the program chooses CPU,\n",
    "    make sure you installed PyTorch's cuda version, and necessary cuda kernels.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"\n",
    "    Moves data to given device\n",
    "    \"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(val_dl, device)\n",
    "test_dl = DeviceDataLoader(test_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Training a model in a personal computer without a GPU takes time, and it's even harder if you are unsure about the architecture of your model, the hyperparameters like learning rate and the optimizer you use. Therefore I implemented two functions for saving and loading a model. Note that, we are saving not just model but optimizer, epoch number and the history.\n",
    "\n",
    "History is a list of dictionaries which stores loss and accuracy values of train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, history, save_path):\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"history\": history\n",
    "    }, save_path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, load_path):\n",
    "    checkpoint = torch.load(load_path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    history = checkpoint[\"history\"]\n",
    "    return model, optimizer, epoch, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, model, optimizer, train_loader, val_loader, history=[], save_path=\"./m.pth\"):\n",
    "    pre_epoch = len(history)\n",
    "\n",
    "    for epoch in range(pre_epoch, epochs + pre_epoch):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "\n",
    "        for batch in train_loader:\n",
    "            loss, acc = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            train_accs.append(acc)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Update history\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['train_acc'] = torch.stack(train_accs).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "        # Save the current progress\n",
    "        save_checkpoint(model, optimizer, epoch, history, save_path)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters may vary depending on the network and dataset, for this dataset and for this architecture these values do okay. For example one using a standart learning rate (0.001) didn't yield a proper result, because model was struggling to learn. That's why I decreased it by time and tried again and again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 3\n",
    "lr = 5.5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't be training the network again, because I already have trained and saved it. That's why I will load from my disk, and continue with that. Since we also saved the history, we still will be able to loss and accuracy graphs. If you want to train it, feel free the run the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = to_device(MyNet(num_classes), device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "history = fit(num_epochs, model, optimizer, train_dl, val_dl, save_path=\"../models/mynet-v1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = to_device(MyNet(num_classes), device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "model, optimizer, num_epochs, history = load_checkpoint(model, optimizer, \"../models/mynet-v1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to understand the progress, it's a good idea to plot loss and accuracy over epochs. We can get insight about the learning progress of our model. For example, down below, we can see the `validation loss` is near minimum and it seems it won't be going to decrease even more. If we had trained the model for not **30** epochs but **50** epochs, we would see the dramatic increase for `validation loss`. It means that our model is starting to learn by heart. It's no coincidence that the `training loss` is always decreasing, that's the point of using an optimizer. It will almost always choose the parameters that will result in less error. Therefore we need more \"fair\" method to see progress, which is `validation dataset`. We do not upgrade the model parameters while predicting over validation dataset, with this way model never learns from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(history):\n",
    "    train_acc = [x[\"train_acc\"] for x in history]\n",
    "    val_acc = [x[\"val_acc\"] for x in history]\n",
    "    plt.plot(train_acc, \"-bo\")\n",
    "    plt.plot(val_acc, \"-ro\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend([\"Training\", \"Validation\"])\n",
    "    plt.title(\"Accuracy vs. No. of epochs\");\n",
    "\n",
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    train_losses = [x.get(\"train_loss\") for x in history]\n",
    "    val_losses = [x[\"val_loss\"] for x in history]\n",
    "    plt.plot(train_losses, \"-bo\")\n",
    "    plt.plot(val_losses, \"-ro\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend([\"Training\", \"Validation\"])\n",
    "    plt.title(\"Loss vs. No. of epochs\");\n",
    "\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def show_confusion_matrix(model, test_dl, classes):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for images, labels in test_dl:\n",
    "        out = model(images)\n",
    "        prob, preds  = torch.max(out, dim=1)\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(labels)\n",
    "\n",
    "    accuracy = 100 * sum(int(pred == true) for pred, true in zip(y_pred, y_true)) / len(y_pred)\n",
    "    print(f\"Accuracy: {round(accuracy, 3)}\")\n",
    "\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None],\n",
    "                         index = [i for i in classes],\n",
    "                         columns = [i for i in classes])\n",
    "    plt.figure(figsize = (8,5))\n",
    "    sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(model, test_dl, dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Predictions\n",
    "\n",
    "Let's grab some pictures from our test dataset, see the predictions vs. true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, img):\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    yb = model(xb)\n",
    "    prob, preds  = torch.max(yb, dim=1)\n",
    "    return dataset.classes[preds[0].item()]\n",
    "\n",
    "def show_predictions(model, ds, num_samples):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        row, col = divmod(i, 3)\n",
    "        index = torch.randint(len(ds.dataset), size=(1,)).item()\n",
    "        img, label = ds.dataset[index]\n",
    "        pred = predict_image(model, img)\n",
    "        axes[row, col].imshow(img.permute(1, 2, 0))\n",
    "        axes[row, col].set_title(f\"Label: {ds.dataset.classes[label]} | Pred: {pred}\")\n",
    "        axes[row, col].axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(model, test_ds, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some external images and save them into specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Google-Cardboard.jpg/1200px-Google-Cardboard.jpg\",\n",
    "                           \"../resources/external-images/cardboard.jpg\");\n",
    "urllib.request.urlretrieve(\"https://upload.wikimedia.org/wikipedia/commons/a/a8/Empty_tin_can2009-01-19.jpg\",\n",
    "                           \"../resources/external-images/can.jpg\");\n",
    "urllib.request.urlretrieve(\"https://www.wpt-nl.com/images/module_image/img1_800_600_1593777835.jpg\",\n",
    "                           \"../resources/external-images/paper-trash.jpg\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_external_image(image_name):\n",
    "    transformations = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Resize((256, 256), antialias=True),\n",
    "        v2.Normalize(imagenet_mean, imagenet_std)\n",
    "    ])\n",
    "    image = Image.open(Path(f\"../external-images/{image_name}\"))\n",
    "    example_image = transformations(image)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"This is an image of {predict_image(model, example_image)}.\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_external_image(\"../resources/external-images/cardboard.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_external_image(\"../resources/external-images/can.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_external_image(\"../resources/external-images/paper-trash.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Our model is able to classify garbage with around **75-80% accuracy**! It's great to see the model's predictions on the test set. It works alright on external images too.\n",
    "\n",
    "However, it's worth noting that there is still a big gap to improve. I don't think this improvements will be mainly on network, but I think they will be on dataset. When you look at the dataset, we always see the images with white background. This situtation causes for our model to predict everything with brownish, yellowish color as cardboard. This is one of the things I realized after investing hours into this project. It's possible to merge the dataset with other datasets from Kaggle, we can both increase the number of data and variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
